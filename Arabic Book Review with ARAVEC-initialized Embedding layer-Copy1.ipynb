{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset.tsv', sep='\\t',header=0 ,encoding=\"utf-8\",\n",
    "                      names=['rating', 'review_id', 'user_id', 'book_id', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>39428407</td>\n",
       "      <td>1775679</td>\n",
       "      <td>3554772</td>\n",
       "      <td>من أمتع ما قرأت من روايات بلا شك. وحول الشك ت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>32159373</td>\n",
       "      <td>1304410</td>\n",
       "      <td>3554772</td>\n",
       "      <td>رواية تتخذ من التاريخ ،جوًا لها اختار المؤلف ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>442326656</td>\n",
       "      <td>11333112</td>\n",
       "      <td>3554772</td>\n",
       "      <td>إني أقدّر هذه الرواية كثيرا، لسبب مختلف عن أس...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>46492258</td>\n",
       "      <td>580165</td>\n",
       "      <td>3554772</td>\n",
       "      <td>الكاهن الذي أطلق على نفسه اسم هيبا تيمنا بالع...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>25550893</td>\n",
       "      <td>1252226</td>\n",
       "      <td>3554772</td>\n",
       "      <td>\"عزازيل\" هو اسم رواية يوسف زيدان الثانية و ال...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  review_id   user_id  book_id  \\\n",
       "0       4   39428407   1775679  3554772   \n",
       "1       4   32159373   1304410  3554772   \n",
       "2       1  442326656  11333112  3554772   \n",
       "3       5   46492258    580165  3554772   \n",
       "4       5   25550893   1252226  3554772   \n",
       "\n",
       "                                              review  \n",
       "0   من أمتع ما قرأت من روايات بلا شك. وحول الشك ت...  \n",
       "1   رواية تتخذ من التاريخ ،جوًا لها اختار المؤلف ...  \n",
       "2   إني أقدّر هذه الرواية كثيرا، لسبب مختلف عن أس...  \n",
       "3   الكاهن الذي أطلق على نفسه اسم هيبا تيمنا بالع...  \n",
       "4   \"عزازيل\" هو اسم رواية يوسف زيدان الثانية و ال...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.loc[:, ['review', 'rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeArabic(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(r\"ى\", \"ي\", text)\n",
    "    text = re.sub(r\"ة\",\"ه\", text)\n",
    "    text = re.sub(r\"[0-9]|[!؟،؛,-_]\", \" \", text)\n",
    "    text = re.sub(r'\"', \" \", text)\n",
    "    text = re.sub(r'[\\(\\)]', \" \", text)\n",
    "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    \n",
    "    text = re.sub(noise, '', text)\n",
    "    text = text.strip()\n",
    "    return ''.join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['review'] = dataset['review'].apply(normalizeArabic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'من امتع ما قرات من روايات بلا شك  وحول الشك تدندن  عزازيل  بلا هواده  احمد الديب'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['review'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making ratings from 0 to 4 instead 1 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['rating'] = dataset['rating'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(dataset['rating'], 5)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squeezing reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [review for review in dataset['review']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one hot encoding for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34465\n",
      "3112\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(t.word_index)+1\n",
    "print(vocab_size)\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2],\n",
       " [4133],\n",
       " [7],\n",
       " [53],\n",
       " [2],\n",
       " [444],\n",
       " [293],\n",
       " [648],\n",
       " [4134],\n",
       " [474],\n",
       " [8939],\n",
       " [27],\n",
       " [293],\n",
       " [6542],\n",
       " [1384],\n",
       " [3497]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.texts_to_sequences(inpt.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4125, 7, 53, 2, 442, 288, 641, 4126, 469, 8933, 27, 288, 6536, 1374, 3488]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code is to know the maximum review in length ,so that I know the padding nomber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3085"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(sublist.split()) for sublist in reviews]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 3085   \n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading aravec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aravec = gensim.models.Word2Vec.load('full_grams_cbow_100_wiki/full_grams_cbow_100_wiki.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = aravec.wv.get_vector(word)\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "    except KeyError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 2.15736341,  3.08792496,  1.72121239, ...,  0.17832963,\n",
       "        -0.75964814,  2.01433587],\n",
       "       [ 1.71386635,  2.76462722, -0.12731455, ..., -0.10195694,\n",
       "        -0.44580469,  4.97124434],\n",
       "       ...,\n",
       "       [ 1.52720261,  0.99132931,  0.06829713, ..., -0.78637141,\n",
       "         2.80833316, -1.39495456],\n",
       "       [-0.08024251, -0.13364261, -0.04714713, ...,  0.54665828,\n",
       "         0.28098759,  0.15865459],\n",
       "       [-0.20780919, -0.25758916,  0.38040265, ..., -0.17234182,\n",
       "        -1.1358428 , -0.08691123]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting\n",
    "## Note shuffling the data is not correct here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , y_train = padded_docs[:2500] , y[:2500]\n",
    "x_test , y_test = padded_docs[2500:] , y[2500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    2,  4125,     7, ...,     0,     0,     0],\n",
       "       [   14, 14138,     2, ...,     0,     0,     0],\n",
       "       [   59,  1981,    16, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    8,   568,   143, ...,     0,     0,     0],\n",
       "       [  132,    40,  3988, ...,     0,     0,     0],\n",
       "       [ 1935,     3,  4345, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction Using Embedding Layer and building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "model.add(e)\n",
    "# Before embedding each sentence was vector of 3086 dimension, after it each sentence was vector of 100 dim\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 3085, 100)         3446500   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 308500)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 1542505   \n",
      "=================================================================\n",
      "Total params: 4,989,005\n",
      "Trainable params: 1,542,505\n",
      "Non-trainable params: 3,446,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.0982 - accuracy: 0.9768 - val_loss: 2.9240 - val_accuracy: 0.4837\n",
      "Epoch 2/15\n",
      "79/79 [==============================] - 2s 28ms/step - loss: 0.0947 - accuracy: 0.9788 - val_loss: 3.0070 - val_accuracy: 0.4967\n",
      "Epoch 3/15\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.0920 - accuracy: 0.9780 - val_loss: 2.9830 - val_accuracy: 0.4886\n",
      "Epoch 4/15\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.0892 - accuracy: 0.9788 - val_loss: 3.0185 - val_accuracy: 0.4935\n",
      "Epoch 5/15\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.0871 - accuracy: 0.9796 - val_loss: 3.0652 - val_accuracy: 0.5016\n",
      "Epoch 6/15\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.0846 - accuracy: 0.9812 - val_loss: 3.0623 - val_accuracy: 0.4967\n",
      "Epoch 7/15\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.0830 - accuracy: 0.9820 - val_loss: 3.0883 - val_accuracy: 0.4918\n",
      "Epoch 8/15\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.0818 - accuracy: 0.9800 - val_loss: 3.1273 - val_accuracy: 0.4984\n",
      "Epoch 9/15\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.0793 - accuracy: 0.9816 - val_loss: 3.1281 - val_accuracy: 0.4853\n",
      "Epoch 10/15\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.0770 - accuracy: 0.9832 - val_loss: 3.1491 - val_accuracy: 0.4935\n",
      "Epoch 11/15\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.0772 - accuracy: 0.9812 - val_loss: 3.1967 - val_accuracy: 0.5000\n",
      "Epoch 12/15\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.0760 - accuracy: 0.9816 - val_loss: 3.2241 - val_accuracy: 0.5000\n",
      "Epoch 13/15\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.0748 - accuracy: 0.9808 - val_loss: 3.2704 - val_accuracy: 0.4804\n",
      "Epoch 14/15\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.0737 - accuracy: 0.9820 - val_loss: 3.2474 - val_accuracy: 0.4886\n",
      "Epoch 15/15\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 0.0719 - accuracy: 0.9820 - val_loss: 3.2725 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9eb8e01110>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=15, validation_data=(x_test, y_test) ,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Generalization of the model on the test data is bad because of unbalancing of the data ,and trying SMOTE here is meaningless because the representation of the sentence is random because Tokenizer function give each word in each sentence a random number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Arabic Book Review with random-initialized Embedding layer.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17439269, 0.14762722, 0.20788811, 0.18030788, 0.2897841 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another test case nt found in the reviews\n",
    "inpt = 'زفت'\n",
    "inpt = normalizeArabic(inpt)\n",
    "encoded_inpt = [one_hot(inpt, vocab_size)]\n",
    "padded_inpt = pad_sequences(encoded_inpt, maxlen=max_length, padding='post')\n",
    "model.predict(padded_inpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As I said previously the Generalization of the model on the test data is bad because of the unbalancing in the data ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
